---
title: "Exploring *Bacteroides* molecular variation"
engine: knitr
format:
  html:
    code-fold: true
    code-summary: "Show the code"
---

## Connecting *Bacteroides* 16S ASVs to genomic species-level variation

### Retrieving Yan et al 2022 metagenomic reads

We started working with the metagenomic dataset that was published in the [Scientific Data paper by Yan et al 2022](https://www.nature.com/articles/s41597-022-01302-9). This includes a nested subset of 395 metagenomic libraries from the allo-HCT patient population that was profiled with 16S.

We first need to get the list of SRA accessions:

```{r, eval=FALSE}
# Load required library
suppressMessages(library(tidyverse))

# Load table with stool sample metadata
accessions <- suppressMessages(
        read_csv("data/tblASVsamples.csv",
            col_types = cols(
                .default = col_guess(),
                Pool = col_character())) %>%
        filter(!is.na(AccessionShotgun)) %>%
        select(AccessionShotgun, SampleID) %>%
        mutate(SampleID = paste0("s_", SampleID)) %>% # to ensure all filenames start with a letter rather than a number
        distinct())

# Save accessions to misc file
write_tsv(accessions, "misc_files/bacteroides_variation/yan_sd_2022_accessions.txt", col_names = FALSE)
```

Now we can dowdload the reads from SRA, sort them, and rename them with the sample names:
```{bash, eval=FALSE}
# Make directory for raw reads
mkdir -p data/reads/yan_sd_2022

# Fetch, sort and rename reads
sbatch code/scripts/bacteroides_variation/fetch_yan_reads.sh

# Clear SRA cache
rm -r /data1/xavierj/carlos/sra_repo/sra/*
```

### Assembling *Bacteroides* metagenomes with Kraken2 read binning

#### Enriching Kraken database with Baceroides genomes from NCBI

I downloaded all genomes of *Bacteroides* from the NCBI database on April 18th, 2025, and extracted the metadata to a CSV:

```{bash, eval=FALSE}
# Download genomes from NCBI

# NCBI CLI toolkit
conda activate ncbi

# Output directory
bacteroides_genome_dir="analyses/bacteroides_variation/genomes/ncbi"
mkdir -p ${bacteroides_genome_dir}

# Download all Bacteroides genomes
datasets download genome taxon "Bacteroides" --dehydrated --filename ${bacteroides_genome_dir}/bacteroides_genomes.zip

# Extract and rehydrate the files
unzip ${bacteroides_genome_dir}/bacteroides_genomes.zip -d ${bacteroides_genome_dir}
datasets rehydrate --directory ${bacteroides_genome_dir}


# Sort and relabel genome files

# Create directory for all .fna files
mkdir -p analyses/bacteroides_variation/genomes/ncbi/ncbi_dataset/fastas

# Find and move all .fna files to the new directory
find analyses/bacteroides_variation/genomes/ncbi/ncbi_dataset/data/ -not -path "analyses/bacteroides_variation/genomes/ncbi/ncbi_dataset/fastas/*" -type f -name "*.fna" -exec mv {} analyses/bacteroides_variation/genomes/ncbi/ncbi_dataset/fastas/ \;

# Create directory for relabeled fastas
mkdir -p analyses/bacteroides_variation/genomes/ncbi/ncbi_dataset/relabeled_fastas

# Copy and rename files
for file in analyses/bacteroides_variation/genomes/ncbi/ncbi_dataset/fastas/*; do
  base=$(basename "$file")
  new_name="${base%%.*}.fna"
  mv "$file" "analyses/bacteroides_variation/genomes/ncbi/ncbi_dataset/relabeled_fastas/$new_name"
done


# Extract genome metadata to CSV

# Python env
conda activate python3

# Tabulate genome metadata
python software/custom/ncbi_jsonl_to_csv.py analyses/bacteroides_variation/genomes/ncbi/ncbi_dataset/data/assembly_data_report.jsonl analyses/bacteroides_variation/genomes/ncbi/ncbi_dataset/genome_metadata.csv

# Switch to R env
conda activate r-4.4.2
```

This downloaded 15,412 genomes. I filtered them to include only the ones with checkM completeness > 90% and checkM contamination < 5%. This resulted in 7,136 genomes:

```{r, eval=FALSE}
# Load genome metadata file
sampled_genomes <- read_csv("analyses/bacteroides_variation/genomes/ncbi/ncbi_dataset/genome_metadata.csv") %>%
  filter(checkm_completeness >= 90 & checkm_contamination <= 5) %>% 
  mutate(file = str_remove(accession, "\\..*$")) %>%
  select(file, taxid)

# Save CSV
write_csv(sampled_genomes, "analyses/bacteroides_variation/genomes/ncbi/ncbi_dataset/sampled_genomes.csv", col_names = FALSE)
```

I had to add the NCBI Tax ID to the headers of the selected genomes before adding them to the kraken database:

```{bash, eval=FALSE}
sh software/custom/taxid_to_headers.sh \
  -c analyses/bacteroides_variation/genomes/ncbi/ncbi_dataset/sampled_genomes.csv \
  -d analyses/bacteroides_variation/genomes/ncbi/ncbi_dataset/relabeled_fastas \
  -o analyses/bacteroides_variation/genomes/ncbi/ncbi_dataset/kraken_header_fastas > \
  analyses/bacteroides_variation/genomes/ncbi/ncbi_dataset/header_trim.log
```

After this, I created a kraken db that included the *Bacteroides* genomes:

```{bash, eval=FALSE}
sbatch code/scripts/bacteroides_variation/build_kraken_db_18042025.sh
```

#### Read binning and assembly

Finally, I classified and the reads witht he custom Kraken2 database, extracted the reads classified as *Bacteroides*, and assembled *Bacteroides* metagenomes with metaSPAdes:

```{bash, eval=FALSE}
sbatch code/scripts/bacteroides_variation/classify_assemble.sh
```

To save time during binning, I filtered the assembled contigs and kept only the ones >= 1000 bp:

```{bash, eval=FALSE}
sbatch code/scripts/bacteroides_variation/bacteroides_filter_contigs.sh
```

### Assembling full metagenomes

I preprocessed the reads with fastp and generated a metagenomic assembly with metaSPAdes for each metagenomic library:

```{bash, eval=FALSE}
# Metagenome
sbatch code/scripts/bacteroides_variation/yan_sd_2022_metaspades.sh #211 (653) and 367 (FMT.0111) did not work after trying many memory configurations
# Plasmids
#sbatch code/scripts/bacteroides_variation/yan_sd_2022_metaplasmid.sh
```

To save time during binning, I filtered the assembled contigs and kept only the ones >= 1000 bp:

```{bash, eval=FALSE}
sbatch code/scripts/bacteroides_variation/yan_sd_2022_filter_contigs.sh
```

### Recovering *Bacteroides* MAGs

I am now going to run two binning strategies (single and multi-sample) and two binning algorithms (VAMB and COMEBin) starting from the two assemblies generated prior.

#### Single-sample binning

I started with single-sample binning with VAMB:

```{bash, eval=FALSE}
# Single sample binning
#sbatch code/scripts/bacteroides_variation/yan_sd_2022_comebin_single.sh

# VAMB binning
sbatch code/scripts/bacteroides_variation/yan_sd_2022_vamb_single.sh
sbatch code/scripts/bacteroides_variation/bacteroides_vamb_single.sh
```

#### Multi-sample binning

I started with multi-sample binning with VAMB. First, I generated the files with the list of samples sequenced with metagenomes for each patient:

```{r, eval=FALSE}
# Load required library
suppressMessages(library(tidyverse))

# Load Patient sample data
patient_samples <- suppressMessages(
        read_csv("data/tblASVsamples.csv",
            col_types = cols(
                .default = col_guess(),
                Pool = col_character())) %>%
        filter(!is.na(AccessionShotgun)) %>%
        select(PatientID, SampleID) %>%
        mutate(SampleID = paste0("s_", SampleID)) %>% # to ensure all filenames start with a letter rather than a number
        distinct())

# Create directory for patient sample lists
dir.create("analyses/yan_sd_2022/binning/patient_samples", recursive = TRUE, showWarnings = FALSE)

# Save SampleIDs for each PatientID to separate .txt files
unique_patient_ids <- unique(patient_samples$PatientID)
for (patient_id in unique_patient_ids) {
  sample_ids <- patient_samples$SampleID[patient_samples$PatientID == patient_id]
  file_path <- paste0("analyses/yan_sd_2022/binning/patient_samples/", patient_id, ".txt")
  writeLines(sample_ids, file_path)
}
```

These files are needed to arrange assembly concatenation and read mapping. After that, I concatenated the assemblies from each patient, mapped the reads, and ran multi-sample VAMB:

```{bash, eval=FALSE}
sbatch code/scripts/bacteroides_variation/yan_sd_2022_vamb_multi.sh
sbatch code/scripts/bacteroides_variation/bacteroides_vamb_multi.sh
```

#### MAG quality assesment

First, I needed to compile the bins obtained from the different strategies I used. 

The single-sample bins from the full metagenomes obtained by VAMB:

```{bash, eval=FALSE}
# Define paths
VAMB_DIR="analyses/yan_sd_2022/binning/vamb/single"
COMPILED_DIR="analyses/yan_sd_2022/binning/compiled"

# Create the compiled directory if it doesn't exist
mkdir -p ${COMPILED_DIR}

# Loop through all bins in the VAMB output directories
find ${VAMB_DIR} -type f -name "*.fna" | while read -r bin_path; do
    # Extract sample name and bin number from the path
    sample_name=$(echo ${bin_path} | awk -F'/' '{print $(NF-3)}')  # Extract sample name (e.g., s_1042A)
    bin_number=$(basename ${bin_path} .fna)                      # Extract bin number (e.g., 3)

    # Construct the new filename
    new_filename="${sample_name}_${bin_number}_meta_vamb_single.fna"

    # Copy the bin to the compiled directory with the new name
    cp ${bin_path} ${COMPILED_DIR}/${new_filename}
done
```

The single-sample bins from the targeted *Bacteroides* metagenomes obtained by VAMB:

```{bash, eval=FALSE}
# Define paths
BIN_DIR="analyses/bacteroides_variation/binning/vamb/single"
COMPILED_DIR="analyses/bacteroides_variation/binning/compiled"

# Create the compiled directory if it doesn't exist
mkdir -p ${COMPILED_DIR}

# Loop through all bins in the VAMB output directories
find ${BIN_DIR} -type f -name "*.fna" | while read -r bin_path; do
    # Extract sample name and bin number from the path
    sample_name=$(echo ${bin_path} | awk -F'/' '{print $(NF-3)}')  # Extract sample name (e.g., s_1042A)
    bin_number=$(basename ${bin_path} .fna)                      # Extract bin number (e.g., 3)

    # Construct the new filename
    new_filename="${sample_name}_${bin_number}_bacteroides_vamb_single.fna"

    # Copy the bin to the compiled directory with the new name
    cp ${bin_path} ${COMPILED_DIR}/${new_filename}
done
```

The multi--sample bins from the full metagenomes obtained by VAMB:

```{bash, eval=FALSE}
# Define paths
VAMB_DIR="analyses/yan_sd_2022/binning/vamb/multi"
COMPILED_DIR="analyses/yan_sd_2022/binning/compiled"

# Loop through all bins in the VAMB output directories
find ${VAMB_DIR} -type f -name "*.fna" | while read -r bin_path; do
    # Extract sample name and bin number from the path
    sample_name=$(echo ${bin_path} | awk -F'/' '{print $NF}' | awk -F'_' '{print $1"_"$2}')
    bin_number=$(basename ${bin_path%%.fna} | awk -F'/' '{print $NF}' | awk -F'_' '{print $4}')  # Extract bin number (e.g., 3)

    # Construct the new filename
    new_filename="${sample_name}_${bin_number}_meta_vamb_multi.fna"

    # Copy the bin to the compiled directory with the new name
    cp ${bin_path} ${COMPILED_DIR}/${new_filename}
done
```

The single-sample bins from the targeted *Bacteroides* metagenomes obtained by VAMB:

```{bash, eval=FALSE}
# Define paths
BIN_DIR="analyses/bacteroides_variation/binning/vamb/multi"
COMPILED_DIR="analyses/bacteroides_variation/binning/compiled"

# Loop through all bins in the VAMB output directories
find ${BIN_DIR} -type f -name "*.fna" | while read -r bin_path; do
    # Extract sample name and bin number from the path
    sample_name=$(echo ${bin_path} | awk -F'/' '{print $NF}' | awk -F'_' '{print $1"_"$2}')
    bin_number=$(basename ${bin_path%%.fna} | awk -F'/' '{print $NF}' | awk -F'_' '{print $4}')

    # Construct the new filename
    new_filename="${sample_name}_${bin_number}_bacteroides_vamb_multi.fna"

    # Copy the bin to the compiled directory with the new name
    cp ${bin_path} ${COMPILED_DIR}/${new_filename}
done
```

Then, I used GTDB-tk to get identities of the bins 

```{bash, eval=FALSE}
# Classify bins with GTDB
sbatch code/scripts/bacteroides_variation/yan_sd_2022_gtdb.sh #6815076
sbatch code/scripts/bacteroides_variation/bacteroides_gtdb.sh #6828018
```

And then I copied the ones from the family Bacteroidaceae:

```{bash, eval=FALSE}
# Define input and output paths
GTDB_FILES=("analyses/yan_sd_2022/binning/gtdb/gtdbtk.bac120.summary.tsv" "analyses/bacteroides_variation/binning/gtdb/gtdbtk.bac120.summary.tsv")
BIN_DIRS=("analyses/yan_sd_2022/binning/compiled" "analyses/bacteroides_variation/binning/compiled")
OUTPUT_DIR="analyses/bacteroides_variation/genomes/mags"

# Create the output directory if it doesn't exist
mkdir -p ${OUTPUT_DIR}
# Loop through GTDB files
for i in "${!GTDB_FILES[@]}"; do
  gtdb_file="${GTDB_FILES[$i]}"
  bin_dir="${BIN_DIRS[$i]}"
  
  # Extract filenames of bins classified in the family Bacteroidaceae and copy them
  awk -F'\t' 'NR > 1 && $2 ~ /f__Bacteroidaceae/ {print $1}' "${gtdb_file}" | while read -r bin_file; do
    full_path="${bin_dir}/${bin_file}.fna"
    if [[ -f ${full_path} ]]; then
      cp "${full_path}" "${OUTPUT_DIR}/"
    else
      echo "File ${full_path} not found, skipping..."
    fi
  done
done
```

On these Bacteroidaceae MAGs, I ran CheckM2:

```{bash, eval=FALSE}
sbatch code/scripts/bacteroides_variation/bacteroides_checkm2.sh
```

#### Comparing *Bacteroides* MAG recovery strategy

I compared the number of near-complete (NC, >90% completeness and <5% contamination) MAGs of the genus *Bacteroides* recovered with different assembly and binning strategies:

```{r}
# Load required library and custom functions
suppressMessages(library(tidyverse))
source("code/rfunctions/data_helpers.r")

# Load and merge GTBD results
targeted_gtdb <- suppressMessages(read_tsv("analyses/bacteroides_variation/binning/gtdb/gtdbtk.bac120.summary.tsv"))
meta_gtdb <- suppressMessages(read_tsv("analyses/yan_sd_2022/binning/gtdb/gtdbtk.bac120.summary.tsv"))
full_gtdb <- suppressMessages(bind_rows(targeted_gtdb, meta_gtdb))

# Load CheckM2 results
checkm <- suppressMessages(read_tsv("analyses/bacteroides_variation/genomes/checkm2/quality_report.tsv"))

# Compile all MAG data
bacteroides_mag_data <- checkm %>%
  mutate(assembly_strategy = case_when(
            str_detect(Name, "bacteroides") ~ "targeted",
            str_detect(Name, "meta") ~ "meta"
          ),
          binning_strategy = case_when(
            str_detect(Name, "single") ~ "single",
            str_detect(Name, "multi") ~ "multi"
          ),
          sample = str_split(Name, "_", simplify = TRUE)[, 2],
        ) %>%
  left_join(full_gtdb, by = c("Name" = "user_genome")) %>%
  left_join(mutate(patient_samples, SampleID = str_remove(SampleID, "^s_")), by = c("sample" = "SampleID")) %>%
  mutate(species = str_remove(classification, ".*s__")) %>%
  filter(Completeness >= 90 & Contamination <= 5 & (str_detect(classification, "g__Bacteroides|s__Phocaeicola vulgatus")))

# Summarize MAG recovery by method combination
mag_recovery_summary <- bacteroides_mag_data %>%
  group_by(assembly_strategy, binning_strategy) %>%
  summarise(n_mags = n(), .groups = "drop")

# Barplot for number of MAGs recovered
ggplot(mag_recovery_summary, aes(x = interaction(assembly_strategy, binning_strategy), y = n_mags)) +
  geom_bar(stat = "identity", position = "dodge", fill = "gray70") +
  geom_text(aes(label = n_mags), vjust = -0.5, size = 3) +
  labs(
    x = "Assembly and binning strategy",
    y = "Number of NC MAGs"
  ) +
  custom_theme
```

I noticed that the sets of samples and species recovered with different strategies were not overlapping, so I also compared the proportion of unique sample-species combinations recovered with each strategy:

```{r}
# Calculate percentage of recovered sample-classification combinations
total_combinations <- bacteroides_mag_data %>%
  distinct(sample, classification) %>%
  nrow()

percentage_recovery_summary <- bacteroides_mag_data %>%
  group_by(assembly_strategy, binning_strategy) %>%
  summarise(
    recovered_combinations = n_distinct(paste(sample, classification)),
    percentage_recovered = (recovered_combinations / total_combinations) * 100,
    .groups = "drop"
  )

# Barplot for percentage of recovered combinations
ggplot(percentage_recovery_summary, aes(x = interaction(assembly_strategy, binning_strategy), y = percentage_recovered)) +
  geom_bar(stat = "identity", position = "dodge", fill = "gray70") +
  geom_text(aes(label = sprintf("%.1f%%", percentage_recovered)), vjust = -0.5, size = 3) +
  labs(
    x = "Aseembly and read mapping strategy",
    y = "Percent of sample-species combinations recovered"
  ) +
  custom_theme
ggsave(filename = "document/plots/mag_recovery_comparison.pdf", width = 5, height = 6)
```

Based on this, it is clear that the **best MAG recovery strategy is to assemble the full metagenome and do multi-sample binning grouping by patient**.

### Extracting 16S sequences

I created a HMMER profile of 16S rRNA sequences using the seed alignment of the [Rfam RF00177](https://rfam.org/family/RF00177#tabview=tab2), and extracted the 16S sequences from the targeted and metagenomic assemblies:

```{bash, eval=FALSE}
#mkdir -p hmms/16S
conda activate hmmerseqkit
hmmbuild hmms/16S/RF00177_16S.hmm hmms/16S/RF00177.stockholm.txt

# Test extraction tool
sh software/custom/nhmmer_extract.sh -p hmms/16S/RF00177_16S.hmm \
  -e 0 \
  -i analyses/bacteroides_variation/genomes/kraken_binning/bacteroides_metaspades/s_1042A/contigs.fasta \
  -s s_1042A \
  -o test.fa \
  -t hits.txt \
  -c 1

# Extract 16S from targeted and untargeted assemblies
sbatch code/scripts/bacteroides_variation/extract_targeted_16S.sh
sbatch code/scripts/bacteroides_variation/extract_meta_16S.sh
```

After that, I compiled all the 16S sequences recovered with both approaches:

```{bash, eval=FALSE}

# Compiling 16S from target assemblies

# Set paths
TARGET_OUTPUT_DIR="analyses/bacteroides_variation/16S/targeted"
TARGET_COMPILED_DIR="analyses/bacteroides_variation/16S/compiled"
TARGET_COMPILED_FILE="${TARGET_COMPILED_DIR}/all_targeted_16S.fasta"

# Create the compiled directory if it doesn't exist
mkdir -p ${TARGET_COMPILED_DIR}

# Initialize the compiled file
> ${TARGET_COMPILED_FILE}

# Loop through all sample directories and merge existing 16S sequences
for target_sample_dir in ${TARGET_OUTPUT_DIR}/*; do
  if [[ -d ${target_sample_dir} ]]; then
    target_fasta_file="${target_sample_dir}/16S_sequences.fasta"
    if [[ -f ${target_fasta_file} ]]; then
      echo "Merging ${target_fasta_file} into ${TARGET_COMPILED_FILE}..."
      cat ${target_fasta_file} >> ${TARGET_COMPILED_FILE}
    else
      echo "No 16S_sequences.fasta found in ${target_sample_dir}, skipping..."
    fi
  fi
done


# Compiling 16S from metagenomic assemblies

# Set paths
META_OUTPUT_DIR="analyses/bacteroides_variation/16S/meta"
META_COMPILED_DIR="analyses/bacteroides_variation/16S/compiled"
META_COMPILED_FILE="${META_COMPILED_DIR}/all_meta_16S.fasta"

# Initialize the compiled file
> ${META_COMPILED_FILE}

# Loop through all sample directories and merge existing 16S sequences
for meta_sample_dir in ${META_OUTPUT_DIR}/*; do
  if [[ -d ${meta_sample_dir} ]]; then
    meta_fasta_file="${meta_sample_dir}/16S_sequences.fasta"
    if [[ -f ${meta_fasta_file} ]]; then
      echo "Merging ${meta_fasta_file} into ${META_COMPILED_FILE}..."
      cat ${meta_fasta_file} >> ${META_COMPILED_FILE}
    else
      echo "No 16S_sequences.fasta found in ${meta_sample_dir}, skipping..."
    fi
  fi
done

# Compile all 16S
cat ${TARGET_COMPILED_FILE} ${META_COMPILED_FILE} > ${META_COMPILED_DIR}/compiled_16S.fasta
```

Finally, I classified the sequences with Kraken2 using the SILVA database and filtered the Bacteroides sequences:

```{bash, eval=FALSE}
# Build the SILVA DB for Kraken
sbatch code/scripts/bacteroides_variation/build_silva_db.sh

# Classify and filter Bacteroides 16S
sbatch code/scripts/bacteroides_variation/classify_bacteroides_16S.sh
```

***Notes:***

  - Why do I consistently get 2 16S copies (one from a *Bacteroides* and one from what seems to be a *Blautia*) from the metagenomes assembled from binned reads??
    Potentially because some species classified as *Bacteroides* (e.g., *B. galacturonicus*) are actually part of the Clostridiales and closely realted to *Lachnospira*, which is in the same family as *Blautia*.
  - There are 234 samples for which I did not extract a 16S sequence with an e-value of 0. Some of these seem to be cases where the 16S assembly is fragmented. See `cat analyses/bacteroides_variation/16S/targeted/s_FMT.0187P/16S_hits.tbl` for an example. Might be able to stich them together if absolutely needed.
  - Check which metagenome assemblies failed and rerun with higher memory

### Matching 16S ASVs with full-length 16S sequences

I wanted to know if the 16S sequences I extracted encompass the all the variation in the ASVs and what the correspondence is. I started by printing the Bacteroides 16S ASVs to a fasta:

```{r, eval=FALSE}
 bacteroides_asvs <- read_csv("data/tblASVtaxonomy_silva132_v4v5_filter.csv") %>%
  filter(Genus == "Bacteroides") %>%
  select(ASV, Sequence)

# Save Bacteroides sequences in FASTA format
writeLines(
  paste0(">", bacteroides_asvs$ASV, "\n", bacteroides_asvs$Sequence),
  "analyses/bacteroides_variation/16S/compiled/bacteroides_asvs.fasta"
)
```

Then, I did a BLASTn search of *Bacteroides* 16S ASVs against a database of the full length 16S extracted from the metagenomes:

```{bash, eval=FALSE}
sbatch code/scripts/bacteroides_variation/blast_bacteroides_asvs_to_meta.sh
```

I also did a BLASTn search of the *Bacteroides* 16S sequences against the NCBI database to get a preliminary idea of the diversity:

```{bash, eval=FALSE}
# Run blast against NCBI
sbatch code/scripts/bacteroides_variation/blast_bacteroides_16S_ncbi.sh
```

The BLAST search only returned the NCBI TaxIDs of the hits, so I retrieved the correspoding species names:

```{bash, eval=FALSE}
# NCBI CLI
conda activate ncbi

# Input file with TaxIDs (one per line)
TAXID_FILE="analyses/bacteroides_variation/16S/compiled/taxid_hits.txt"

# Output file for results
OUTPUT_FILE="analyses/bacteroides_variation/16S/compiled/taxid_species.tsv"

# Initialize the output file
> $OUTPUT_FILE

# Loop through each TaxID in the file
while read -r TAXID; do
    # Run the datasets command for the current TaxID
    datasets summary taxonomy taxon $TAXID | \
    grep -o '"species":{"id":[0-9]*,"name":"[^"]*"}' | \
    sed -E "s/.*\"id\":[0-9]+,\"name\":\"([^\"]+)\".*/${TAXID}\t\1/" >> $OUTPUT_FILE
done < $TAXID_FILE
```

With this information, I was able to summarize the performance of the two strategies for 16S sequence assembly from metagenomes with respect to the number matching 16S ASVs:

```{r}
# Load the BLAST results and species ids
meta_to_ncbi_blast <- read_tsv("analyses/bacteroides_variation/16S/compiled/bacteroides_ncbi_blast.txt")
asv_to_meta_blast <- read_tsv("analyses/bacteroides_variation/16S/compiled/bacteroides_asv_to_meta_blast.txt", col_names = F)
colnames(asv_to_meta_blast) <- c("asv", "hit", "pident", "qlen", "slen", "alignment_length", "mismatches", "gaps")
taxid_species <- read_tsv("analyses/bacteroides_variation/16S/compiled/taxid_species.tsv", col_names = F)

# Wrangle NCBI blast results to get IDs for the metagenomic 16S seqs
meta_to_ncbi_curated_blast <- meta_to_ncbi_blast %>%
  left_join(taxid_species, by = c("sscinames" = "X1")) %>%
  rename(species = X2) %>%
  group_by(qseqid) %>%
  slice_max(order_by = pident, with_ties = FALSE, n = 1) %>%
  ungroup() %>%
  mutate(method = str_extract(qseqid, "meta|targeted"))

# Add meta IDs to ASV blast results
asv_to_meta_map <- asv_to_meta_blast %>%
  left_join(select(meta_to_ncbi_curated_blast, qseqid, species, sacc, method), by = c(hit = "qseqid")) %>%
  group_by(asv) %>%
  slice_max(order_by = pident, with_ties = TRUE, n = 1)

# Summarize recovery by method
recovery_summary <- asv_to_meta_map %>%
  mutate(bacteroides_species = str_detect(species, "Bacteroides")) %>% # Flag rows with "Bacteroides" in species
  group_by(method) %>%
  summarise(
    asvs_mismatches_0 = n_distinct(asv[mismatches == 0]),
    asvs_mismatches_1 = n_distinct(asv[mismatches == 1]),
    asvs_mismatches_2 = n_distinct(asv[mismatches == 2]),
    asvs_mismatches_gt_2 = n_distinct(asv[mismatches > 2]),
    asvs_mismatches_0_bacteroides = n_distinct(asv[mismatches == 0 & bacteroides_species]),
    asvs_mismatches_1_bacteroides = n_distinct(asv[mismatches == 1 & bacteroides_species]),
    asvs_mismatches_2_bacteroides = n_distinct(asv[mismatches == 2 & bacteroides_species]),
    asvs_mismatches_gt_2_bacteroides = n_distinct(asv[mismatches > 2 & bacteroides_species])
  ) %>%
  ungroup()

# View the summary table
print(recovery_summary)
```

These results indicate that **assembling the full metagenomic library is a better strategy to recover near-full-length 16S sequences that match 16S ASVs from the amplicon data**.

Nevertheless, a couple of 16S ASVs had exact matches with 16S sequences recovered with the targeted strategy but not with the meta strategy: 

```{r}
# Check which unique ASVs with mismatches == 0 in "targeted" are not in "meta"
targeted_asvs <- asv_to_meta_map %>%
  filter(method == "targeted", mismatches == 0) %>%
  pull(asv) %>%
  unique()

meta_asvs <- asv_to_meta_map %>%
  filter(method == "meta", mismatches == 0) %>%
  pull(asv) %>%
  unique()

not_recovered_asvs <- setdiff(targeted_asvs, meta_asvs)

if (length(not_recovered_asvs) > 0) {
  message("The following ASVs were not recovered by 'meta':")
  print(not_recovered_asvs)
} else {
  message("All ASVs with mismatches == 0 in 'targeted' were recovered by 'meta'.")
}
```

Finally, I summarized how many different species match each 16S ASV based on their similarity with the full-length 16S sequences. I will reassess these correspondences once I have a phylogeny. I also summarized the prevalence of each ASV in the HCT patient population:

```{r}
# Create a summary table with distinct species for each ASV
asv_species_summary <- asv_to_meta_map %>%
  group_by(asv, mismatches) %>%
  summarise(
    n_species = n_distinct(species), # Count distinct species
    species = paste(unique(species), collapse = ", ") # Concatenate distinct species names
  ) %>%
  ungroup()

# View the resulting table
print(asv_species_summary)

# Save the species summary as an .RData file
save(asv_species_summary, file = "analyses/processed_data/asv_species_summary.RData")
```

### Gnerating ASV identity table

With the information above, I generated a table with the identities of all ASVs in the full dataset. For ASVs with 0 mismatches to a single *Bacteroides/Phocaeicola* species, I set the identity at the species level. For all other ASVs, I sent the identity at the genus level:

```{r}
# Table with species IDs for single-matching Bacteroides ASVs
bacteroides_species_asvs <- asv_species_summary %>%
  filter(mismatches == 0 & n_species == 1 & !str_detect(species, "uncultured.*")) %>%
  select(asv, species)

# Table with all ASV genus identities and Bacteroides speices
asv_genera_bacteroides_species <- suppressMessages(
  read_csv("data/tblASVtaxonomy_silva132_v4v5_filter.csv")
) %>%
  select(ASV, Genus) %>%
  left_join(bacteroides_species_asvs, by = c("ASV" = "asv")) %>%
  mutate(identity = if_else(is.na(species), Genus, species)) %>%
  select(ASV, identity)

# Save the table as R object for modeling

# Create the directory if it doesn't exist
if (!dir.exists("analyses/processed_data")) {
    dir.create("analyses/processed_data", recursive = TRUE)
}

# Save the table object as an .RData file
save(asv_genera_bacteroides_species, file = "analyses/processed_data/asv_genera_bacteroides_species.RData")
```

## Polysaccharide degradation potential variation

The goal here was to determine whether genomic potential for carbohydrate degradation was variable between species and consistent within species.

### CGC prediction

I predicted CAZyme gene clusters (CGC) using the dbCAN3 annotation pipeline on the Bacteroidaceae MAGs I recovered previously:

```{bash, eval=FALSE}
sbatch code/scripts/bacteroides_variation/bacteroides_dbcan.sh #16607040
```

Then, I explored how the MAGs clusterd in (i) a UMAP space by PUL content (predicted from BLAST to PUL database):

```{r}
# Load UMAP library and set seed
library(umap)
set.seed(42)

# Find all PUL predictions files recursively
tsv_files <- list.files(
  path = "analyses/bacteroides_variation/cgc/dbcan",
  pattern = "substrate_prediction.tsv",
  recursive = TRUE,
  full.names = TRUE
)

# Read and combine all files, adding parent directory as identifier
pul_data <- purrr::map_dfr(tsv_files, function(f) {
  parent_dir <- basename(dirname(f))
  readr::read_tsv(
    f,
    col_types = cols(
      .default = col_guess(),
      bitscore = col_double(),
      `dbCAN-sub substrate score` = col_character()
    )
  ) %>%
    dplyr::mutate(mag = parent_dir)
  }) %>%
  filter(!is.na(PULID)) %>%
  left_join(select(bacteroides_mag_data, Name, species, PatientID), by = c("mag" = "Name")) %>%
  filter(!is.na(PatientID))

# Create a binary presence/absence matrix of PULID by MAG
# Create binary presence/absence matrix of PULID by MAG, keeping species column
pul_matrix <- pul_data %>%
  distinct(mag, PULID, species) %>%
  mutate(present = 1) %>%
  pivot_wider(names_from = PULID, values_from = present, values_fill = 0)

# Select only PUL columns for UMAP (exclude 'mag' and 'species')
pul_cols <- setdiff(colnames(pul_matrix), c("mag", "species"))
pul_umap_input <- as.matrix(pul_matrix[, pul_cols])

# Run UMAP

pul_umap <- umap(pul_umap_input)

# Add UMAP coordinates to the data frame
pul_matrix$UMAP1 <- pul_umap$layout[, 1]
pul_matrix$UMAP2 <- pul_umap$layout[, 2]

# Assign a specific color to each species value
species_levels <- unique(pul_matrix$species)
library(RColorBrewer)
species_colors <- setNames(brewer.pal(max(3, min(length(species_levels), 12)), "Paired")[seq_along(species_levels)], species_levels)

# Plot UMAP colored by species with explicit color assignment
ggplot(pul_matrix, aes(x = UMAP1, y = UMAP2, color = species)) +
  geom_point(alpha = 0.8, size = 2) +
  scale_color_manual(values = species_colors, breaks = species_levels) +
  labs(
    x = "UMAP 1",
    y = "UMAP 2",
    color = "Species"
  ) +
  custom_theme
```

```{r}
# Load phylogenetics packages
suppressMessages(library(ape))
suppressMessages(library(ggtree))
suppressMessages(library(treeio))
suppressMessages(library(ggtreeExtra))
suppressMessages(library(aplot))

# Count the number of times each PUL appears in each MAG
pul_count_matrix <- pul_data %>%
  group_by(mag, PULID) %>%
  summarise(copy_number = n(), .groups = "drop") %>%
  pivot_wider(names_from = PULID, values_from = copy_number, values_fill = 0)

# Set rownames to MAGs and convert to matrix class
pul_count_matrix <- as.data.frame(pul_count_matrix)
rownames(pul_count_matrix) <- pul_count_matrix$mag
pul_count_matrix <- as.matrix(pul_count_matrix[,-1])

# Calculate PUL distance matrix
pul_dist_matrix <- dist(pul_count_matrix, method = "euclidean")

# Estimate hierarchical clustering
pul_hc <- hclust(pul_dist_matrix, method = "average")

# Convert hclust to phylo
pul_tree <- as.phylo(pul_hc)

# Table with MAG species
mag_species <- pul_data %>%
  distinct(mag, species)

# Table with PUL substrates
pul_substrate <- pul_data %>%
  distinct(PULID, `dbCAN-PUL substrate`)

# Create a custom palette of 13 distinct colors
custom_13 <- c(
  "#1b9e77", "#d95f02", "#7570b3", "#e7298a", "#66a61e",
  "#e6ab02", "#a6761d", "#666666", "#a6cee3", "#fb9a99",
  "#fdbf6f", "#b2df8a", "#cab2d6"
)

# Ensure species is a factor and assign colors
mag_species$species <- factor(mag_species$species)
species_levels <- levels(mag_species$species)
species_colors <- setNames(custom_13[seq_along(species_levels)], species_levels)

# Plot the tree with colored squares for species
pul_tree_plot <- ggtree(pul_tree) +
  geom_fruit(
    data = mag_species,
    geom = geom_tile,
    mapping = aes(y = mag, fill = species),
    width = 0.05,
    offset = 0.02,
    color = NA
  ) +
  scale_fill_manual(values = species_colors, name = "Species") #+
#  theme(legend.position = "none")
  
# Summarize the number of PULs for each MAG and substrate
pul_bubble <- pul_data %>%
  group_by(mag, substrate = `dbCAN-PUL substrate`) %>%
  summarise(count = n(), .groups = "drop")

# Define the order of carbohydrate substrates by complexity
carb_order <- c(
  "capsule polysaccharide synthesis", "capsule polysaccharide degradation", "mucin", "host glycan",
  "glycosaminoglycan", "human milk oligosaccharide", "pectin", "arabinogalactan", "arabinoxylan", "xyloglucan",
  "galactomannan", "glucomannan", "alginate", "carrageenan", "agarose",
  "starch", "glycogen", "cellulose", "chitin", "beta-glucan", "beta-mannan", "alpha-mannan", "alpha-galactan",
  "arabinan", "xylan", "fructan",
  "beta-galactooligosaccharide", "beta-mannooligosaccharide", "raffinose", "melibiose", "cellobiose", "trehalose",
  "galactose", "fucose", "ribose", "beta-glucoside", "levoglucosan"
)

# Keep only substrates present in the data
carb_order <- carb_order[carb_order %in% pul_bubble$substrate]
pul_bubble$substrate <- factor(pul_bubble$substrate, levels = carb_order)

# Bubble plot: MAGs vs carbohydrate substrates, bubble size/color = PUL count
polysacc_bubblemap <- ggplot(pul_bubble, aes(x = substrate, y = mag, size = count, color = count)) +
  geom_point(alpha = 0.6) +
  scale_size_continuous(name = "PUL count") +
  scale_color_viridis_c(name = "PUL count", direction = -1) +
  theme_minimal(base_size = 10) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    axis.text.y = element_blank(), # Remove Y axis labels
    axis.title.y = element_blank() # Remove Y axis label
  ) +
  labs(
    x = "Carbohydrate substrate (ordered by complexity)"
  )

# Join cluter and bubblemap plots
polysacc_bubblemap %>% insert_left(pul_tree_plot)

# Save the plot
ggsave(filename = "document/plots/bacteroides_pul_bubblemap.pdf",
  height = 12, width = 10)
```